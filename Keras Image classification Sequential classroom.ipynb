{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small image classification project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first lets import tensor and keras and confirm versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and define data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use MNIST, which is a famous/frequently used training db for images. \n",
    "#This version Fashion MNIST includes 70000 grey scale images 28x28 px,10 classes, representing fashion items \n",
    "#with keras you can load MNIST, Fashion MNIST, housing data set OOTB \n",
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set is split for us into train and test\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every image is an array of the pixels , stored as integers (0-255)\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whats in the test data ? lets look at some of the images in a plot\n",
    "# summarize loaded dataset\n",
    "print('X_Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    " # define subplot\n",
    " pyplot.subplot(330 + 1 + i)\n",
    " # plot raw pixel data\n",
    " pyplot.imshow(X_test[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we need to create a validation data set of 5000 images\n",
    "#& scale the data set to suit the Gradient Descent training method (divide by 255.0)\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "#note, we do not need to scale the dependent variable (ie, what we are going to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classes of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we know there are 10 classes, but there are currently no labels in the data set (0-9) \n",
    "# as we need labels for the validation lets add them\n",
    "# its a standard list which you can look up on the keras documentation and kaggle\n",
    "class_names = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate the classes using the images in the above plot \n",
    "class_names[y_test[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model (NN) - Sequential API - in this case a single stack of linear layers, connected sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential() # create model\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) #first layer - flattens each image into a 1d array (Pre-pr)\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) # add a 300 neuron dense layer\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) # add a second layer of neurons\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) # add a third layer of 10 neurons (one per class)\n",
    "\n",
    "# note : dense layers often have tonnes of parameters ( first dense layer = 235500 p's)\n",
    "# this adds flexibility, but risks overfit if training data not substantial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu : \n",
    "\n",
    "In order to use stochastic gradient descent with backpropagation of errors to train deep neural networks, an activation function is needed that looks and acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.\n",
    "\n",
    "The function must also provide more sensitivity to the activation sum input and avoid easy saturation.\n",
    "\n",
    "The rectified linear activation function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less. ie IF input > 0 return input, else 0 \n",
    "\n",
    "The rectifier function mostly looks and acts like a linear activation function.\n",
    "\n",
    "In general, a neural network is easier to optimize when its behavior is linear or close to linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax : \n",
    "    \n",
    "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
    "\n",
    "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
    "\n",
    "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model - optional to specify metrics for compute in training and evaluation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", # we have few labels and they are exclusive (0-9, trousers/ankle boots)\n",
    "optimizer=\"sgd\", #using stochastic gradient descent to train model\n",
    "metrics=[\"accuracy\"]) # as its a classifer, we are interested in accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at each stage we can see the number of instancves processed, the mean training time per sample\n",
    "#plus the loss and accuracy or any other metrics requested in the last stage \n",
    "\n",
    "# your computer is likely working hard at this point\n",
    "# - note that for complex models, more CPU is needed than an average computer can supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even plot the learning curve \n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "# training and val accuracy slowly increase, training and val loss steadily decrease \n",
    "# if you arent satisfied with model performance, go back and tune parameters - eg no of layers, neruons, activation fx, epochs, batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on test set to see accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
