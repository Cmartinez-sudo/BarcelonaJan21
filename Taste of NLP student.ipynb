{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Lecture notebook - trying out a few NLP tools together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Lexical analysis with and NLTK over a text document (MS word), converted with docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install tools using !pip or other method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "\n",
    "#once installed, you need to import and run the download for popular packages which will be saved locally\n",
    "import nltk \n",
    "#nltk.download()\n",
    "#ensure you see the message True if downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these nltk tools allow us to search the text for a specific word \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docx2txt\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytextfile = docx2txt.process(\"rugby_story.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX 1 NLP on a body of text - example : news article in a word doc, scraped text from the web/api, or a url open request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets use nltk to separate the individual  words out of the text\n",
    "tokens= word_tokenize(mytextfile)\n",
    "print(tokens[:100]) #the first 100 words in the text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many words do we have?\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a NLTK text from the list of words to allow for further linguistic processing \n",
    "textlist= Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have changed the data type \n",
    "type(textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for words that frequently appear together \n",
    "textlist.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using regex we can seek key terms in our text \n",
    "textlist.findall(r\"<safety> (<.*>) <seriously>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find fragments containing key words - note, an index is built so searching is faster next time for this word\n",
    "textlist.concordance(\"injury\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can even achieve easy lexical visualisation of the text \n",
    "\n",
    "textlist.dispersion_plot([\"safety\",\"player\",\"brain\",\"injury\",\"dementia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have missed the most important stage of NLP! \n",
    "Lets stop and do some basic cleaning operations on our raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets deal with the punctuation - we know the text included words like can't, it's and symbols like '-', \n",
    "# so we remove everything non alphabetic \n",
    "words=[word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, lets make everything lower case \n",
    "lowerwords = [word.lower() for word in words]\n",
    "print(lowerwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, removing stopwords (the, a, is) to leave us with cleaner set of usable text \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words=[w for w in lowerwords if not w in stop_words]\n",
    "print(clean_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count freq of words in the text \n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdistrugby = FreqDist(clean_words)\n",
    "fdistrugby.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX 2 - reviews and sentiment analysis - basic - with textblob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 - add some sentiment ready data \n",
    "# step 2 - clean and tokenise as before (we will skip this step today)\n",
    "# step 3 - calculate sentiment with text blob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "clothingdf=pd.read_csv('Womens Clothing E-Commerce Reviews.csv')\n",
    "clothingdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothingdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning - lets focus on two key columns, so I need to keep populated rows \n",
    "clothingdf.dropna(subset=['Review Text','Division Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothingdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob \n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score for each statement on a scale negative to positive \n",
    "# polarity -> VERY -ve = -1  VERY +ve = +1 \n",
    "# example - input some text\n",
    "\n",
    "TextBlob(\"I love this dress!\").sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lambda to pick up sentiment across all of review text column, adding a new column to the df\n",
    "clothingdf['Review_Polarity'] = clothingdf['Review Text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothingdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at each divisions/departments review polarity \n",
    "clothingdf.groupby(['Division Name','Department Name'])['Review_Polarity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3 - using SpaCy as a headstart with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one of the biggest challenges of working with text is interpreting context, whats the subject/object etc.  \n",
    "\n",
    "#!pip install -U spacy\n",
    "#install model - language and size \n",
    "\n",
    "import spacy \n",
    "#import en_core_web_sm\n",
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"I saw a man on the hill with a telescope. He looks like Jesus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisation \n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: The original word text.\n",
    "Lemma: The base form of the word.\n",
    "POS: The simple UPOS part-of-speech tag.\n",
    "Tag: The detailed part-of-speech tag.\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "Shape: The word shape â€“ capitalization, punctuation, digits.\n",
    "is alpha: Is the token an alpha character?\n",
    "is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse and tag each word with context using the model installed\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets explore visually the word links in the text we have provided, to see what spacy is adding to our analysis\n",
    "\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see if spacy can spot the named entity \n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text analysis - similarity of words detected using vectors - requires bigger model to be loaded though\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"man woman hill telescope wkls\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)#out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allows us to compare each word to another, for similarity, using those vectors \n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Ex using SpaCy to detect and rank  named entities from social data (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets= pd.read_csv('ever_trump.csv')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #obviously this text needs cleaning operations but lets simply create tokens using spacy ready for analysis \n",
    "\n",
    "tokens = nlp(''.join(str(tweets.text.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "items = [x.text for x in tokens.ents]\n",
    "Counter(items).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on named persons in the tweets \n",
    "person_list= [] # empty list \n",
    "\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "        person_list.append(ent.text) # using loop to fill the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_counts = Counter(person_list).most_common(20)\n",
    "df_person =pd.DataFrame(person_counts, columns = ['text', 'count'])\n",
    "df_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.plot.barh(x='text', y='count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
